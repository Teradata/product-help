Le rapport d'évaluation permet d'évaluer un modèle et de marquer un modèle champion en fonction de ses performances. Vous pouvez afficher le rapport d'évaluation qui met en évidence les performances du modèle sous la forme de certaines mesures et comparer les modèles sur la base des valeurs de mesures.

Le rapport d'évaluation du modèle affiche les zones suivantes :

-   Détails de la version de modèle


-   Mesures clés


-   Mesures


-   Graphiques de performances


-   Actions


**Détails de la version de modèle** répertorie tous les détails de la version de modèle, des tâches d'entraînement et d'évaluation. Affichage des détails suivants lié à la tâche d'entraînement :

-   **ID de version de modèle**

    Spécifie l'ID de version de modèle. Vous pouvez sélectionner le lien ID de version de modèle pour accéder à la page **Cycle de vie des versions de modèle**.


-   **ID de tâche d'évaluation**

    Spécifie l'ID de tâche d'évaluation. Vous pouvez sélectionner le lien ID de tâche pour accéder aux détails de la tâche.


-   **Date d'évaluation**

    Spécifie la date d'évaluation.


-   **ID d'ensemble de données**

    Affiche l'ID de l'ensemble de données d'entraînement de la tâche. Sélectionnez le lien **ID d'ensemble de données**pour consulter les détails de l'ensemble de données.


-   **Nom de l'ensemble de données**

    Affiche le nom de l'ensemble de données d'entraînement de la tâche.


-   **Hyperparamètres**

    Spécifie les hyperparamètres définis pour exécuter la tâche, y compris eta et max_depth.


**Mesures clés** affiche les mesures clés que vous marquez dans la zone Mesures, qui peut inclure une liste de mesures de performances. Mesures clés Vous pouvez marquer certaines mesures **Mesures clés** pour y accéder facilement.

**Mesures** répertorie les mesures de performances et leurs valeurs pour la version actuelle du modèle, ce qui peut inclure une liste de mesures telles que **Marquer comme mesure clé** l'exactitude, le **Mesures clés** rappel, la précision et le score F1.

-   **Précision**

    Rapport entre le nombre de prédictions correctes et le nombre total d'échantillons d'entrée.


-   **Rappel**

    Nombre de résultats positifs corrects divisé par le nombre de tous les échantillons pertinents (ceux qui auraient dû être identifiés comme positifs).


-   **Précision**

    Nombre de résultats positifs corrects divisé par le nombre de résultats positifs prédits par le classificateur.


-   **Score-F1**

    Moyenne harmonique entre la précision et le rappel. La plage du score F1 est (0,1). Elle vous indique le niveau de précision de votre classificateur (le nombre d'instances qu'il classe correctement), ainsi que sa robustesse (il ne manque pas un nombre important d'instances).


**Graphiques de performances** affiche un certain nombre de graphiques de performances basés sur différentes mesures, notamment la matrice de confusion, la courbe ROC et l'importance des caractéristiques SHAP. Suivez visuellement les performances du modèle à l'aide de ces graphiques et déterminez si vous souhaitez marquer le modèle comme Champion.

-   **Matrice de confusion**

    Matrice N x N qui évalue les performances du modèle, où N est le nombre de classes cibles. La matrice compare les valeurs cibles réelles à celles prédites par le modèle d'apprentissage automatique.


-   **Courbe ROC**

    Ce graphique résume le compromis entre le taux de vrais positifs et le taux de faux positifs pour un modèle prédictif utilisant différents seuils de probabilité.


-   **Importance des caractéristiques SHAP**

    Ce graphique est basé sur l'ampleur des attributions de caractéristiques.


Les**Actions** utilisent le rapport d'évaluation du modèle pour effectuer l'une des actions suivantes sur la version actuelle du modèle.

-   **Approuver** la version de modèle.


-   **Refuser** la version de modèle.


-   **Marquer/annuler le marquage comme Champion** marque/annule le marquage de la version de modèle en tant que Champion sur la base de ses performances.


-   **Afficher la dérive du modèle** ouvre la page Dérive du modèle sur laquelle vous pouvez surveiller les performances du modèle.


