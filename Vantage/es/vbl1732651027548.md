Utilice el informe de evaluación para evaluar un modelo y marcar un modelo destacado en función de su rendimiento. Puede ver el informe de evaluación que destaca el rendimiento del modelo en forma de ciertas métricas y comparar modelos en función de los valores de las métricas.

El informe de evaluación del modelo muestra las siguientes áreas:

-   Detalles de versión del modelo


-   Métricas clave


-   Métricas


-   Gráficas de rendimiento


-   Acciones


**Detalles de versión del modelo** indica todos los detalles de la versión del modelo y los trabajos de entrenamiento y evaluación. Se muestran los siguientes detalles relacionados con el trabajo de entrenamiento:

-   **ID de versión de modelo**

    Especifica el ID de versión del modelo. Puede seleccionar el enlace del ID de versión del modelo para ir a la página **Ciclo de vida de la versión del modelo**.


-   **ID de trabajo de evaluación**

    Especifica el ID de trabajo de evaluación. Puede seleccionar el enlace del ID de trabajo para ir a los detalles del trabajo.


-   **Fecha de evaluación**

    Especifica la fecha de evaluación.


-   **ID del conjunto de datos**

    Muestra el ID del conjunto de datos de entrenamiento utilizado para entrenar el trabajo. Seleccione el enlace **ID del conjunto de datos** para ver los detalles del conjunto de datos.


-   **Nombre del conjunto de datos**

    Muestra el nombre del conjunto de datos de entrenamiento utilizado para entrenar el trabajo.


-   **Hiperparámetros**

    Especifica los hiperparámetros definidos para ejecutar el trabajo, incluidos eta y max_depth.


**Métricas clave** muestra las métricas clave que se marcan en el área Métricas, que puede incluir una lista de métricas de rendimiento. Puede marcar algunas métricas como **Métricas clave** para acceder fácilmente a ellas. Todas las métricas clave se mostrarán en esta área.

**Métricas** indica las métricas de rendimiento y sus valores para la versión del modelo actual, que puede incluir una lista de métricas como precisión, recuperación y puntuación F1. Utilice la opción **Marcar como métrica clave** para marcar las métricas clave para mostrar en el área **Métricas clave**. Una lista de métricas de rendimiento comunes es:

-   **Precisión**

    La relación entre el número de predicciones correctas y el número total de muestras de entrada.


-   **Recuperación**

    El número de resultados positivos correctos dividido por el número de todas las muestras relevantes (todas las muestras que deberían haber sido identificadas como positivas).


-   **Precisión**

    El número de resultados positivos correctos dividido por el número de resultados positivos previstos por el clasificador.


-   **Puntuación F1**

    La media armónica entre precisión y recuperación. El rango para la puntuación F1 es (0,1). Indica el nivel de precisión del clasificador (cuántas instancias clasifica correctamente) y su nivel de solidez (no omite una cantidad significativa de instancias).


**Gráficas de rendimiento** muestra una serie de gráficos de rendimiento basados en diferentes métricas, incluida la matriz de confusión, la curva ROC y la importancia de características SHAP. Utilice estos gráficos para supervisar visualmente el rendimiento del modelo y decidir si desea marcar el modelo como Destacado.

-   **Matriz de confusión**

    Una matriz N x N que evalúa el rendimiento del modelo, donde N es el número de clases objetivo. La matriz compara los valores objetivo reales con los predichos por el modelo de aprendizaje automático.


-   **Curva ROC**

    Este gráfico resume la compensación entre la tasa de verdaderos positivos y la tasa de falsos positivos para un modelo predictivo que utiliza diferentes umbrales de probabilidad.


-   **Importancia de características SHAP**

    Este gráfico se basa en la magnitud de las atribuciones de características.


**Acciones** utilice el informe de evaluación del modelo para realizar cualquiera de las siguientes acciones en la versión del modelo actual.

-   **Aprobar** la versión del modelo.


-   **Rechazar** la versión del modelo.


-   **Marcar o desmarcar como Destacada** marca o desmarca la versión del modelo como destacada en función de su rendimiento.


-   **Ver la desviación del modelo** abre la página Desviación del modelo, donde se puede supervisar el rendimiento del modelo.


