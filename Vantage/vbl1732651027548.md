Use the Evaluation report to evaluate a model and mark a champion model based on its performance. You can view the evaluation report that highlights the model performance in the form of certain metrics and compare models based on the metric values.

The model evaluation report displays the following areas:

-   Model version details


-   Key metrics


-   Metrics


-   Performance charts


-   Actions


**Model version details** lists down all the details of the model version, training and evaluation jobs. The following details display related to the training job:

-   **Model version ID**

    Specifies the model version ID. You can select the model version ID link to go to the **Model version lifecycle** page.


-   **Evaluation job ID**

    Specifies the evaluation job ID. You can select the Job ID link to go to the job's details.


-   **Evaluation date**

    Specifies the evaluation date.


-   **Dataset ID**

    Displays the training dataset ID used to train the job. Select the **Dataset ID** link to see the dataset details.


-   **Dataset name**

    Displays the training dataset name used to train the job.


-   **Hyper parameters**

    Specifies the hyper parameters defined to run the job including eta and max_depth.


**Key metrics** displays the key metrics that you mark in the Metrics area which can include a list of performance metrics. You can mark some of the metrics as **Key metrics** to easily access them. All the key metrics will display in this area.

**Metrics** lists the performance metrics and their values for the current model version which can include a list of metrics including Accuracy, Recall, Precision, F1 score. Use the **Mark as key metric** option to mark the key metrics for display in the **Key metrics** area. A list of common performance metrics is:

-   **Accuracy**

    The ratio of the number of correct predictions to the total number of input samples.


-   **Recall**

    The number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive).


-   **Precision**

    The number of correct positive results divided by the number of positive results predicted by the classifier.


-   **F1-score**

    The Harmonic Mean between precision and recall. The range for F1 Score is (0,1). It tells you how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances).


**Performance charts** displays a number of performance charts based on different metrics including Confusion matrix, ROC curve, and SHAP feature importance. Use these charts to monitor model performance visually and decide if you want to mark the model as Champion.

-   **Confusion matrix**

    An N x N matrix that evaluates model performance, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model.


-   **ROC curve**

    This chart summarizes the trade-off between the true positive rate and false positive rate for a predictive model using different probability thresholds.


-   **SHAP Feature Importance**

    This chart is based on magnitude of feature attributions.


**Actions** use the model evaluation report to perform any of the following actions on the current model version.

-   **Approve** the model version.


-   **Reject** the model version.


-   **Mark/Unmark as Champion** mark/unmarks the model version as Champion based on its performance.


-   **View model drift** opens the Model drift page where you can monitor the model performance.


