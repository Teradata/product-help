<div class="nested0" aria-labelledby="ariaid-title1" topicindex="1" topicid="kro1507742711392" id="kro1507742711392"><h1 class="title topictitle1" id="ariaid-title1">XGBoost (ML Engine)</h1><div class="body conbody">
<p class="p">The XGBoost function takes a training data set and uses gradient boosting to create a strong classifying model that can be input to the function <a href="jtv1543874390081.md#yuw1507743806357">XGBoostPredict (ML Engine)</a>. The function supports input tables in both dense and sparse format.</p></div><div class="topic reference nested1" aria-labelledby="ariaid-title2" topicindex="2" topicid="vhd1507742770820" xml:lang="en-us" lang="en-us" id="vhd1507742770820">
<h2 class="title topictitle2" id="ariaid-title2">XGBoost Syntax</h2><div class="body refbody"><div class="section" id="vhd1507742770820__section_N1000E_N1000C_N10001">
<h3 class="title sectiontitle">Version <span>2.19</span></h3><pre class="pre codeblock" xml:space="preserve"><code>SELECT * FROM XGBoost (
  <span>ON { <var class="keyword varname">table</var> | <var class="keyword varname">view</var> | (<var class="keyword varname">query</var>) }</span> AS InputTable
  [ <span>ON { <var class="keyword varname">table</var> | <var class="keyword varname">view</var> | (<var class="keyword varname">query</var>) }</span> AS AttributeTable ]
  [ OUT TABLE OutputTable (<var class="keyword varname">output_table</var>) ]
  USING
  ResponseColumn ('<var class="keyword varname">response_column</var>')
  [ NumericInputs ({ '<var class="keyword varname">numeric_input_column</var>' | <var class="keyword varname">numeric_input_column_range</var> }[,...]) ]
  [ CategoricalInputs ({ '<var class="keyword varname">categorical_input_column</var>' | <var class="keyword varname">categorical_input_column_range</var> }[,...]) ]
  [ LossFunction ({ 'softmax' | 'binomial' }) ]
  [ PredictionType ('classification') ]
  [ AttributeNameColumn ('<var class="keyword varname">attribute_name_column</var>') ]
  [ AttributeValueColumn ('<var class="keyword varname">attribute_value_column</var>') ]
  [ RegularizationLambda (<var class="keyword varname">lambda</var>) ]
  [ ShrinkageFactor (<var class="keyword varname">shrinkage</var>) ]
  [ ColumnSubSampling (<var class="keyword varname">sample_fraction</var>) ]
  [ IDColumn ('<var class="keyword varname">id_column</var>') ] 
  [ NumBoostedTrees (<var class="keyword varname">num_trees</var>) ]
  [ IterNum (<var class="keyword varname">iterations</var>) ]
  [ MinNodeSize (<var class="keyword varname">min_node_size</var>) ]
  [ MaxDepth (<var class="keyword varname">max_depth</var>) ]
  [ Variance (<var class="keyword varname">variance</var>) ]
  [ Seed (<var class="keyword varname">seed</var>) ]
) AS <var class="keyword varname">alias</var>;</code></pre></div></div><div class="related-links"><div class="linklistheader"><p></p><b>Related Information</b></div>
<ul class="linklist linklist relinfo"><div class="linklistmember"><a href="ndv1557782188375.md">Column Specification Syntax Elements</a></div></ul></div></div><div class="topic reference nested1" aria-labelledby="ariaid-title3" topicindex="3" topicid="qwo1507742774621" xml:lang="en-us" lang="en-us" id="qwo1507742774621">
<h2 class="title topictitle2" id="ariaid-title3">XGBoost Syntax Elements</h2><div class="body refbody"><div class="section" id="qwo1507742774621__section_N10011_N1000E_N10001"><dl class="dl parml"><dt class="dt pt dlterm">OutputTable</dt><dd class="dd pd">[Optional] Specify the name for the model table that the function outputs.</dd><dd class="dd pd ddexpand">Default: xgboost_model in the current schema</dd><dt class="dt pt dlterm">ResponseColumn</dt><dd class="dd pd">Specify the name of the InputTable column that contains the response variable for each data point in the training data set.</dd><dt class="dt pt dlterm">NumericInputs</dt><dd class="dd pd">[Not for sparse format input data. With dense format input data, required if you omit CategoricalInputs.] Specify the names of the InputTable columns to treat as the numeric predictor variables. These variables must be numeric values.</dd><dt class="dt pt dlterm">CategoricalInputs</dt><dd class="dd pd">[Not for sparse format input data. With dense format input data, required if you omit NumericInputs.] Specify the names of the InputTable columns to treat as the categorical predictor variables. These variables can be either numeric or VARCHAR values.</dd><dd class="dd pd ddexpand">For information about columns that you must identify as numeric or categorical, see <a href="uxa1540574678350.md">Identification of Numeric and Categorical Columns</a>.</dd><dt class="dt pt dlterm">LossFunction</dt><dd class="dd pd">[Optional] Specify the learning task and corresponding learning objective:
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="qwo1507742774621__table_xst_bqz_fdb" class="table" frame="border" border="1" rules="all"><div class="caption"></div><colgroup span="1"><col style="width:50%" span="1"></col><col style="width:50%" span="1"></col></colgroup><thead class="thead" style="text-align:left;"><tr class="row"><th class="entry cellrowborder" style="vertical-align:top;" id="d13523e233" rowspan="1" colspan="1">Option</th><th class="entry cellrowborder" style="vertical-align:top;" id="d13523e235" rowspan="1" colspan="1">Description</th></tr></thead><tbody class="tbody"><tr class="row"><td class="entry cellrowborder" style="vertical-align:top;" headers="d13523e233" rowspan="1" colspan="1"><code class="ph codeph">'softmax'</code> (Default)</td><td class="entry cellrowborder" style="vertical-align:top;" headers="d13523e235" rowspan="1" colspan="1">For multiple-class classification.</td></tr><tr class="row"><td class="entry cellrowborder" style="vertical-align:top;" headers="d13523e233" rowspan="1" colspan="1"><code class="ph codeph">'binomial'</code></td><td class="entry cellrowborder" style="vertical-align:top;" headers="d13523e235" rowspan="1" colspan="1">Negative binomial likelihood, for binary classification.</td></tr></tbody></table></div></dd><dt class="dt pt dlterm">PredictionType</dt><dd class="dd pd">[Optional] The function supports only 'classification'.</dd><dt class="dt pt dlterm">AttributeNameColumn</dt><dd class="dd pd">[Required if the input data set is in sparse format] Specify the name of the InputTable column that contains the names of the attributes of the input data set.</dd><dt class="dt pt dlterm">AttributeValueColumn</dt><dd class="dd pd">[Required if the input data set is in sparse format] Specify the name of the InputTable column that contains the values of the attributes of the input data set.</dd><dt class="dt pt dlterm">RegularizationLambda</dt><dd class="dd pd">[Optional] Specify the L2 regularization that the loss function uses while boosting trees. The <var class="keyword varname">lambda</var> is a DOUBLE PRECISION value in the range [0, 100000]. The higher the <var class="keyword varname">lambda</var>, the stronger the regularization effect. The value 0 specifies no regularization.</dd><dd class="dd pd ddexpand">Default: 100000</dd><dt class="dt pt dlterm">ShrinkageFactor</dt><dd class="dd pd">[Optional] Specify the learning rate (weight) of a learned tree in each boosting step. After each boosting step, the algorithm multiplies the learner by <var class="keyword varname">shrinkage</var> to make the boosting process more conservative. The <var class="keyword varname">shrinkage</var> is a DOUBLE PRECISION value in the range (0, 1]. The value 1 specifies no shrinkage.</dd><dd class="dd pd ddexpand">Default: 0.1</dd><dt class="dt pt dlterm">ColumnSubSampling</dt><dd class="dd pd">Specify the fraction of features to subsample during boosting. The <var class="keyword varname">sample_fraction</var> is a DOUBLE PRECISION value in the range (0, 1].</dd><dd class="dd pd ddexpand">Default: 1.0 (no subsampling)</dd><dt class="dt pt dlterm">IDColumn</dt><dd class="dd pd">[Required with NumBoostedTrees or Seed, of if InputTable is in sparse format; optional otherwise.] Specify the name of the InputTable column that contains a unique identifier for each data point in the test data set.</dd><dt class="dt pt dlterm">NumBoostedTrees</dt><dd class="dd pd">[Optional] Requires IDColumn. Specify the number of parallel boosted trees. The <var class="keyword varname">num_trees</var> is an INTEGER value in the range [1, 10000]. If <var class="keyword varname">num_trees</var> is greater than 1, each boosting operates on a sample of the input data, and the function estimates sample size (number of rows) using this formula:</dd><dd class="dd pd ddexpand"><var class="keyword varname">sample_size</var> = <var class="keyword varname">total_number_of_input_rows</var> / <var class="keyword varname">number_of_trees</var></dd><dd class="dd pd ddexpand">The <var class="keyword varname">sample_size</var> must fit in a vworker memory.</dd><dd class="dd pd ddexpand">A higher <var class="keyword varname">num_trees</var> value might improve function run time but decrease prediction accuracy.</dd><dd class="dd pd ddexpand">Default: Function determines <var class="keyword varname">num_trees</var> from data set size and  available memory.</dd><dt class="dt pt dlterm">IterNum</dt><dd class="dd pd">[Optional] Specify the number of iterations (rounds) to boost the weak classifiers. The <var class="keyword varname">iterations</var> must be an INTEGER in the range [1, 100000].</dd><dd class="dd pd ddexpand">Default: 10</dd><dt class="dt pt dlterm">MinNodeSize</dt><dd class="dd pd">[Optional] Specify a decision-tree stopping criterion, the minimum size of any node within each decision tree. If the size of any node becomes less than <var class="keyword varname">min_node_size</var>, the algorithm stops looking for splits. The <var class="keyword varname">min_node_size</var> must be an INTEGER of at least 1.</dd><dd class="dd pd ddexpand">Default: 1</dd><dt class="dt pt dlterm">MaxDepth</dt><dd class="dd pd">[Optional] Specify the decision-tree stopping criterion that has the greatest effect on function performance, the maximum tree depth. If the tree depth exceeds <var class="keyword varname">max_depth</var>, the algorithm stops looking for splits. A decision tree can grow to 2(<var class="keyword varname">max_depth</var>+1)-1 nodes. The <var class="keyword varname">max_depth</var> must be an INTEGER in the range [1, 100000].</dd><dd class="dd pd ddexpand">Default: 12</dd><dt class="dt pt dlterm">Variance</dt><dd class="dd pd">[Optional] Specify a decision-tree stopping criterion, the minimum variance for any node. If the variance within any node becomes less than <var class="keyword varname">variance</var>, the algorithm stops looking for splits. The <var class="keyword varname">variance</var> is a nonnegative DOUBLE PRECISION value.</dd><dd class="dd pd ddexpand">Default: 0</dd><dt class="dt pt dlterm">Seed</dt><dd class="dd pd">[Optional] Specify the random seed the algorithm uses for repeatable results. If you omit Seed, the function uses a faster algorithm but does not ensure repeatability.</dd><dd class="dd pd ddexpand">The <var class="keyword varname">seed</var> must be a LONG value greater than or equal to 1.<div class="note note" id="qwo1507742774621__note_N101EF_N101E6_N101DA_N10017_N10013_N10010_N10001"><span><b>Note</b></span><div class="notebody"> For repeatable results, use both the Seed and UniqueID syntax elements. For more information, see <a href="qym1549987102806.md">Nondeterministic Results and UniqueID Syntax Element</a>.</div></div></dd></dl></div></div></div></div>
